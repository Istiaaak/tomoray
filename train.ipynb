{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a47e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_loader import make_loader, XRays_CT_Dataset\n",
    "\n",
    "\n",
    "from models.unet2d import unet\n",
    "from models.fullmodel import MultiViewCT\n",
    "from utils.metrics import psnr\n",
    "from utils.volume_vis import show_triptych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9aea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLES_DEG = [0, 45, 90, 135, 180]\n",
    "ROOT = \"../dataset\"\n",
    "DRR_SIZE = (128, 128)\n",
    "CT_SIZE  = (128, 128, 128)\n",
    "\n",
    "\n",
    "CKPT_DIR = Path(\"./checkpoints_model_fix\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "EPOCHS        = 2000\n",
    "BASE_LR       = 1e-5\n",
    "WEIGHT_DECAY  = 1e-5\n",
    "N_SAMPLES     = 131072\n",
    "CHUNK_POINTS  = 8192\n",
    "WARMUP_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e99a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c368c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micn/anaconda3/envs/istiak/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready on cuda\n"
     ]
    }
   ],
   "source": [
    "net2d = unet().to(device)\n",
    "model = MultiViewCT(\n",
    "    unet_2d=net2d,\n",
    "    Z=CT_SIZE[0], Y=CT_SIZE[1], X=CT_SIZE[2],\n",
    "    feat_ch=128,       # must match U-Net output channels\n",
    "    pe_dim=9,\n",
    "    inr_hidden=128,\n",
    "    inr_blocks_view=3,\n",
    "    inr_blocks_global=2,\n",
    "    emb_dim=128,\n",
    "    inr_out=1\n",
    ").to(device)\n",
    "\n",
    "print(\"Model ready on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535d7708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 266)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = make_loader(\n",
    "    root=ROOT,\n",
    "    angles_deg=ANGLES_DEG,\n",
    "    drr_size=DRR_SIZE,\n",
    "    ct_size=CT_SIZE,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "subset_loader = DataLoader(\n",
    "    torch.utils.data.Subset(loader.dataset, [i for i in range(200)]),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "len(subset_loader), len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7735f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "optimG = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=BASE_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    \"\"\"\n",
    "    - Linear warmup from 0 -> 1 during WARMUP_EPOCHS.\n",
    "    - Then cosine decay from 1 -> 0.3.\n",
    "    \"\"\"\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return float(epoch + 1) / float(WARMUP_EPOCHS)\n",
    "    else:\n",
    "        t = (epoch - WARMUP_EPOCHS) / max(1, (EPOCHS - WARMUP_EPOCHS))\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * t))  # 1 -> 0\n",
    "        min_factor = 0.3\n",
    "        return min_factor + (1 - min_factor) * cosine\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimG, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a299d2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f721297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(optimG, last_epoch):\n",
    "    return torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimG,\n",
    "        lr_lambda=lr_lambda,\n",
    "        last_epoch=last_epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afadcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    optimG,\n",
    "    scheduler_fn,\n",
    "    CKPT_DIR,\n",
    "    resume_epoch=None,\n",
    "    total_epochs=EPOCHS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or resume the model.\n",
    "\n",
    "    model:      the neural network\n",
    "    optimG:     optimizer instance\n",
    "    scheduler_fn: a function that returns a scheduler given (optimG, start_epoch)\n",
    "    CKPT_DIR:   folder containing checkpoints\n",
    "    resume_epoch:\n",
    "        None  : resume from latest checkpoint if any exists\n",
    "        int>0 : resume from a specific epoch\n",
    "        <=0   : start from scratch\n",
    "    total_epochs:\n",
    "        maximum number of epochs to train\n",
    "    \"\"\"\n",
    "\n",
    "    start_epoch = 0\n",
    "    ckpt_path = None\n",
    "\n",
    "    ckpt_files = sorted(CKPT_DIR.glob(\"model_epoch*.pt\"))\n",
    "\n",
    "    if resume_epoch is None:\n",
    "        # Use the latest checkpoint if any exists\n",
    "        if ckpt_files:\n",
    "            ckpt_path = ckpt_files[-1]\n",
    "\n",
    "    elif isinstance(resume_epoch, int) and resume_epoch > 0:\n",
    "        # Use a specific epoch checkpoint\n",
    "        ckpt_name = f\"model_epoch{resume_epoch:04d}.pt\"\n",
    "        candidate = CKPT_DIR / ckpt_name\n",
    "        if candidate.exists():\n",
    "            ckpt_path = candidate\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {candidate}\")\n",
    "\n",
    "    else:\n",
    "        # resume_epoch <= 0 â†’ start from scratch\n",
    "        ckpt_path = None\n",
    "\n",
    "    if ckpt_path is not None:\n",
    "        print(f\"Resuming from checkpoint: {ckpt_path}\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        optimG.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "        start_epoch = ckpt.get(\"epoch\", 0)\n",
    "\n",
    "    else:\n",
    "        print(\"No checkpoint loaded, starting from scratch.\")\n",
    "        start_epoch = 0\n",
    "\n",
    "    # Recreate scheduler with the correct last_epoch\n",
    "    scheduler = scheduler_fn(optimG, last_epoch=start_epoch - 1)\n",
    "\n",
    "    print(f\"Training will start at epoch {start_epoch+1} / {total_epochs}\")\n",
    "    print(\"Start training\")\n",
    "\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = len(subset_loader)\n",
    "\n",
    "        for batch_idx, batch in enumerate(subset_loader):\n",
    "            views  = batch[\"views\"].to(device, dtype=torch.float32)    # (B, V, 1, H, W)\n",
    "            angles = batch[\"angles\"].to(device, dtype=torch.float32)   # (B, V)\n",
    "            ct     = batch[\"ct\"].to(device, dtype=torch.float32)       # (B, 1, Z, Y, X)\n",
    "\n",
    "            # Handle shape (B, 1, 1, Z, Y, X) : squeeze extra dim\n",
    "            if ct.ndim == 6 and ct.shape[2] == 1:\n",
    "                ct = ct.squeeze(2)\n",
    "\n",
    "            B, _, Z, Y, X = ct.shape\n",
    "            N_vox = Z * Y * X\n",
    "\n",
    "            assert N_vox == model.coords.shape[0], (\n",
    "                f\"Grid mismatch: ct has {N_vox} voxels, \"\n",
    "                f\"coords has {model.coords.shape[0]}\"\n",
    "            )\n",
    "\n",
    "            # Random voxel sampling\n",
    "            n_use = min(N_SAMPLES, N_vox)\n",
    "            perm  = torch.randperm(N_vox, device=device)\n",
    "            idx   = perm[:n_use]\n",
    "\n",
    "            ct_flat = ct.view(B, -1)\n",
    "            gt      = ct_flat[:, idx]\n",
    "\n",
    "            # Forward only on sampled points\n",
    "            optimG.zero_grad(set_to_none=True)\n",
    "            pred = model.forward_points(\n",
    "                views, angles, idx, chunk=CHUNK_POINTS\n",
    "            )\n",
    "\n",
    "            loss = crit(pred, gt)\n",
    "            loss.backward()\n",
    "            optimG.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 20 == 0:\n",
    "                print(\n",
    "                    f\"  epoch {epoch+1} | batch {batch_idx+1}/{n_batches} \"\n",
    "                    f\"| loss={loss.item():.4e}\",\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "\n",
    "        avg_mse  = running_loss / n_batches\n",
    "        avg_psnr = psnr(avg_mse)\n",
    "        current_lr = optimG.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{total_epochs} \"\n",
    "            f\"LR:{current_lr:.2e} | \"\n",
    "            f\"MSE(subset):{avg_mse:.4e} | PSNR~:{avg_psnr:.2f} dB\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Full-volume visualisation every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample = loader.dataset[0]\n",
    "                v = sample[\"views\"].unsqueeze(0).to(device, dtype=torch.float32)\n",
    "                a = sample[\"angles\"].unsqueeze(0).to(device, dtype=torch.float32)\n",
    "                ct_ref = sample[\"ct\"].unsqueeze(0).to(device, dtype=torch.float32)\n",
    "                if ct_ref.ndim == 6 and ct_ref.shape[2] == 1:\n",
    "                    ct_ref = ct_ref.squeeze(2)\n",
    "                pred_full = model(v, a, chunk=CHUNK_POINTS)\n",
    "\n",
    "            show_triptych(\n",
    "                pred_full[0, 0].cpu().numpy(),\n",
    "                ct_ref[0, 0].cpu().numpy(),\n",
    "                title_prefix=f\"Epoch {epoch+1} (full volume)\",\n",
    "            )\n",
    "            model.train()\n",
    "\n",
    "        # Checkpoints\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == total_epochs:\n",
    "            ckpt_path = CKPT_DIR / f\"model_epoch{epoch+1:04d}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimG.state_dict(),\n",
    "                    \"loss\": avg_mse,\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            print(f\"Checkpoint : {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c386d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: checkpoints_model_fix/model_epoch0870.pt\n",
      "Training will start at epoch 871 / 2000\n",
      "Start training\n",
      "  epoch 871 | batch 140/200 | loss=4.5318e-03\r"
     ]
    }
   ],
   "source": [
    "train_model(model, optimG, make_scheduler, CKPT_DIR, resume_epoch=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "istiak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
